{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51e4dc5",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1cec681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (0.24.1)\n",
      "Requirement already satisfied: pandas in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (3.10.6)\n",
      "Requirement already satisfied: numpy in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (2.2.6)\n",
      "Requirement already satisfied: albumentations in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (2.0.8)\n",
      "Requirement already satisfied: scikit-learn in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: filelock in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from albumentations) (1.16.2)\n",
      "Requirement already satisfied: PyYAML in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from albumentations) (6.0.3)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from albumentations) (2.12.4)\n",
      "Requirement already satisfied: albucore==0.0.24 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from albumentations) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from albucore==0.0.24->albumentations) (4.2.3)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision pandas matplotlib numpy albumentations scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "159bad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.utils import make_grid\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be821468",
   "metadata": {},
   "source": [
    "In this notebook, we define the following constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a21a4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = (0.485, 0.456, 0.406)\n",
    "STD = (0.229, 0.224, 0.225)\n",
    "HEIGHT = 32\n",
    "WIDTH = 32\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "BEST_MODEL_PATH = 'best_model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08258b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "26dd920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare class\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, df, data_dir='data', transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # CSV has 'id' column (e.g., 'train/1.jpg' or 'test/4136.jpg')\n",
    "        img_id = row[\"id\"]\n",
    "        img_path = f\"{self.data_dir}/{img_id}\"  # Construct full path\n",
    "        \n",
    "        # Load label if it exists (train.csv has 'label', test.csv doesn't)\n",
    "        if \"label\" in row:\n",
    "            label = int(row[\"label\"])  # 0–99\n",
    "        else:\n",
    "            label = -1  # No label for test set\n",
    "\n",
    "        # load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc054fd3",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "feb7508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),         # resize\n",
    "    transforms.RandomHorizontalFlip(),     # flip\n",
    "    transforms.RandomRotation(10),         # rotate ±10 degrees\n",
    "    transforms.ToTensor(),                 \n",
    "    transforms.Normalize(MEAN, STD)\n",
    "])\n",
    "\n",
    "test_and_val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9227fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full train data set to be split into train and validation sets\n",
    "full_dataset = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "train_df, valid_df = train_test_split(full_dataset, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "train_dataset = CarDataset(train_df, transform=train_transform)\n",
    "val_dataset   = CarDataset(valid_df, transform=test_and_val_transform)\n",
    "test_dataset  = CarDataset(test_df, transform=test_and_val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9417a8",
   "metadata": {},
   "source": [
    "## Method 1: Transfer Learning\n",
    "Here we use a ResNet18 model from torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6953bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rumeza/Projects/university/cpsc599/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# freeze all pretrained layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# replace the final FC layer\n",
    "model.fc = nn.Linear(512, 100) # 100 classes\n",
    "\n",
    "# unfreeze ONLY the new fc layer\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f960cea3",
   "metadata": {},
   "source": [
    "## Define DataLoader for training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5624eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0876f2b",
   "metadata": {},
   "source": [
    "### Select Device for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04734a00",
   "metadata": {},
   "source": [
    "## LOSS Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20caf2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc443c0f",
   "metadata": {},
   "source": [
    "## Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0af0f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "59998d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, loader, criterion, device, phase='Valid'):\n",
    "  \"\"\"Evaluate the performance of a model on a given dataset.\n",
    "\n",
    "  This function calculates the loss and accuracy of the model on the dataset.\n",
    "    It also returns the ground truth labels and the model's predictions.\n",
    "\n",
    "  Args:\n",
    "    model (torch.nn.Module): The model to be evaluated.\n",
    "    loader (torch.utils.data.DataLoader): The data loader for the dataset.\n",
    "    criterion (torch.nn.modules.loss._Loss): The loss function.\n",
    "    device (torch.device): The device (CPU or GPU) where computations will be performed.\n",
    "    phase (str, optional): The phase of evaluation. Defaults to 'Valid'.\n",
    "\n",
    "  Returns:\n",
    "    dict: A dictionary containing the following keys:\n",
    "      'loss' (float): The average loss of the model on the dataset.\n",
    "      'accuracy' (float): The accuracy of the model on the dataset.\n",
    "      'ground_truth' (list): The ground truth labels of the dataset.\n",
    "      'predictions' (list): The model's predictions on the dataset.\n",
    "    \"\"\"\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  ground_truth = []\n",
    "  with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    total  = 0\n",
    "    correct = 0\n",
    "    for batch_index, (images, labels) in enumerate(loader):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      total_loss += loss.item() * images.size(0)\n",
    "      total += images.size(0)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      predictions.extend(preds.cpu().numpy())\n",
    "      ground_truth.extend(labels.cpu().numpy())\n",
    "      correct += (preds == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    loss = total_loss / total\n",
    "    print(f'     {phase} Accuracy={accuracy:<10.4f}  Loss= {loss:<10.4f}')\n",
    "    return {'loss': loss,\n",
    "            'accuracy': accuracy,\n",
    "            'ground_truth': ground_truth,\n",
    "            'predictions': predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1181e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, valid_loader, criterion, optimizer, device,\n",
    "             epochs, best_model_path):\n",
    "  \"\"\"Train a model and evaluate its performance on a validation set.\n",
    "\n",
    "  This function trains a model for a specified number of epochs and evaluates\n",
    "    its performance on a validation set after each epoch. The model with the\n",
    "    lowest validation loss is saved.\n",
    "\n",
    "  Args:\n",
    "    model (torch.nn.Module): The model to be trained.\n",
    "    train_loader (torch.utils.data.DataLoader): The data loader for the training set.\n",
    "    valid_loader (torch.utils.data.DataLoader): The data loader for the validation set.\n",
    "    criterion (torch.nn.modules.loss._Loss): The loss function.\n",
    "    optimizer (torch.optim.Optimizer): The optimization algorithm.\n",
    "    device (torch.device): The device (CPU or GPU) where computations will be performed.\n",
    "    epochs (int): The number of times the learning algorithm will work through the entire training dataset.\n",
    "    best_model_path (str): The path where the best model will be saved.\n",
    "\n",
    "  Returns:\n",
    "    dict: A dictionary containing the 'loss', 'accuracy', 'ground_truth', and 'predictions'\n",
    "            of the model with the lowest validation loss.\n",
    "    \"\"\"\n",
    "  model.train()\n",
    "  best_loss = torch.inf\n",
    "  best_restults = None\n",
    "  for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total  = 0\n",
    "    correct = 0\n",
    "    for batch_index, (images, labels) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      total_loss += loss.item() * images.size(0)\n",
    "      total += images.size(0)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      correct += (preds == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    loss = total_loss / total\n",
    "    print(f'{epoch:<4} Train Accuracy={accuracy:<10.4f}  Loss= {loss:<10.4f}')\n",
    "    results = evaluation(model, valid_loader, criterion, device)\n",
    "    if results['loss'] < best_loss:\n",
    "      torch.save(model, best_model_path)\n",
    "      best_loss = results['loss']\n",
    "      best_restults = results\n",
    "    print()\n",
    "  return best_restults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "307d137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Train Accuracy=0.0450      Loss= 4.5293    \n",
      "     Valid Accuracy=0.0979      Loss= 4.0140    \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m best_restults = \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                         \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBEST_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtraining\u001b[39m\u001b[34m(model, train_loader, valid_loader, criterion, optimizer, device, epochs, best_model_path)\u001b[39m\n\u001b[32m     32\u001b[39m images = images.to(device)\n\u001b[32m     33\u001b[39m labels = labels.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     36\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/university/cpsc599/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/university/cpsc599/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/university/cpsc599/.venv/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/university/cpsc599/.venv/lib/python3.12/site-packages/torchvision/models/resnet.py:268\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m    270\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/university/cpsc599/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/university/cpsc599/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/university/cpsc599/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/university/cpsc599/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_restults = training(model, train_loader, valid_loader, criterion,\n",
    "                         optimizer, device, NUM_EPOCHS, BEST_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
